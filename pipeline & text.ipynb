{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, LeaveOneOut, ShuffleSplit\n",
    "from sklearn.datasets import load_iris, load_digits, load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 0.93333333, 1.        , 0.93333333,\n",
       "       0.93333333, 0.93333333, 0.93333333, 0.93333333, 1.        ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=3)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "cross_val_score(logreg, load_iris().data, load_iris().target, cv = ShuffleSplit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "y = digits.target == 9\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False]), 0.8955555555555555)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "pred_most_frequent = dummy_majority.predict(X_test)\n",
    "np.unique(pred_most_frequent), dummy_majority.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, max_depth=20).fit(X_train, y_train)\n",
    "pred_forest = forest.predict(X_test)\n",
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9755555555555555"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "pred_logreg = logreg.predict(X_test)\n",
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[399,   4],\n",
       "       [  7,  40]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[403,   0],\n",
       "       [ 10,  37]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951048951048951"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state = 0)\n",
    "\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()),(\"svm\",SVC())])\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9812206572769953\n",
      "0.972027972027972\n",
      "{'svm__C': 1, 'svm__gamma': 1}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],'svm__gamma':[0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid = param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.score(X_test, y_test))\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1],\n",
       "       [1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bards_words = [\"The fool doth think he is wise,\", \"but the wise man knows himself to be a fool\"]\n",
    "vect = CountVectorizer().fit(bards_words)\n",
    "vect.transform(bards_words).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the', 'think', 'to', 'wise']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1)).fit(bards_words)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be fool', 'but the', 'doth think', 'fool doth', 'he is', 'himself to', 'is wise', 'knows himself', 'man knows', 'the fool', 'the wise', 'think he', 'to be', 'wise man']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2)).fit(bards_words)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'be fool', 'but', 'but the', 'but the wise', 'doth', 'doth think', 'doth think he', 'fool', 'fool doth', 'fool doth think', 'he', 'he is', 'he is wise', 'himself', 'himself to', 'himself to be', 'is', 'is wise', 'knows', 'knows himself', 'knows himself to', 'man', 'man knows', 'man knows himself', 'the', 'the fool', 'the fool doth', 'the wise', 'the wise man', 'think', 'think he', 'think he is', 'to', 'to be', 'to be fool', 'wise', 'wise man', 'wise man knows']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,3)).fit(bards_words)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Full of (then) unknown actors TSF is a great big cuddly romp of a film.<br /><br />The idea of a bunch of bored teenagers ripping off the local sink factory is odd enough, but add in the black humour that Forsyth & Co are so good at and your in for a real treat.<br /><br />The comatose van driver by itself worth seeing, and the canal side chase is just too real to be anything but funny.<br /><br />And for anyone who lived in Glasgow it\\'s a great \"Oh I know where that is\" film.'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"data/train/\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(text_train[0])\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "\n",
    "df = pd.DataFrame({'X_train': text_train, 'y_train': y_train})\n",
    "X_train = df[df['y_train']!=2]['X_train']\n",
    "y_train = df[df['y_train']!=2]['y_train']\n",
    "\n",
    "review_text = load_files(\"data/test/\")\n",
    "text_test, y_test = review_text.data, review_text.target\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8282399999999999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(X_train)\n",
    "X_train = vect.transform(X_train)\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(LogisticRegression(), X_train, y_train, cv = 5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.82964, {'C': 10}, 0.84212)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C':[ 0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_score_, grid.best_params_, grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8874, {'C': 0.1}, 0.87784)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df[df['y_train']!=2]['X_train']\n",
    "\n",
    "vect = CountVectorizer(min_df=5).fit(X_train)\n",
    "\n",
    "X_train = vect.transform(X_train)\n",
    "X_test = vect.transform(text_test)\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv = 5).fit(X_train, y_train)\n",
    "grid.best_score_, grid.best_params_, grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.88368, {'C': 0.1}, 0.87252)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df[df['y_train']!=2]['X_train']\n",
    "\n",
    "vect = CountVectorizer(min_df = 5, stop_words = 'english').fit(X_train)\n",
    "\n",
    "X_train = vect.transform(X_train)\n",
    "X_test = vect.transform(text_test)\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv = 5).fit(X_train, y_train)\n",
    "grid.best_score_, grid.best_params_, grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89416"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfTransformer: take in the sparse matrix output produced by CountVectorizer and transform it.\n",
    "# TfidfVectorizer: take in the text data and do both the bag-of-words feature extraction and the tf-idf transmation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df = 5, norm = None), LogisticRegression())\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv = 5)\n",
    "X_train = df[df['y_train']!=2]['X_train']\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['poignant', 'disagree', 'instantly', 'importantly', 'lacked',\n",
       "        'occurred', 'currently', 'altogether', 'nearby', 'undoubtedly'],\n",
       "       dtype='<U20'),\n",
       " array(['dominick', 'the', 'victor', 'bridget', 'victoria', 'khouri',\n",
       "        'zizek', 'rob', 'timon', 'titanic'], dtype='<U20'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "X_train = df[df['y_train']!=2]['X_train']\n",
    "X_train = vectorizer.transform(X_train)\n",
    "max_value = X_train.max(axis= 0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "feature_names[sorted_by_tfidf[:10]], feature_names[sorted_by_tfidf[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'and', 'of', 'to', 'this', 'is', 'it', 'in', 'that', 'but',\n",
       "       'for', 'with', 'was', 'as', 'on', 'movie', 'not', 'have', 'one',\n",
       "       'be', 'film', 'are', 'you', 'all', 'at', 'an', 'by', 'so', 'from',\n",
       "       'like', 'who', 'they', 'there', 'if', 'his', 'out', 'just',\n",
       "       'about', 'he', 'or', 'has', 'what', 'some', 'good', 'can', 'more',\n",
       "       'when', 'time', 'up', 'very'], dtype='<U20')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low inverse document frequency: those appear frequently and deemed less important\n",
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "feature_names[sorted_by_idf[:50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worst</th>\n",
       "      <th>waste</th>\n",
       "      <th>awful</th>\n",
       "      <th>bad</th>\n",
       "      <th>boring</th>\n",
       "      <th>poor</th>\n",
       "      <th>poorly</th>\n",
       "      <th>worse</th>\n",
       "      <th>terrible</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>...</th>\n",
       "      <th>fun</th>\n",
       "      <th>today</th>\n",
       "      <th>favorite</th>\n",
       "      <th>amazing</th>\n",
       "      <th>loved</th>\n",
       "      <th>wonderful</th>\n",
       "      <th>best</th>\n",
       "      <th>perfect</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.241149</td>\n",
       "      <td>-0.189228</td>\n",
       "      <td>-0.168824</td>\n",
       "      <td>-0.167629</td>\n",
       "      <td>-0.153832</td>\n",
       "      <td>-0.128468</td>\n",
       "      <td>-0.120763</td>\n",
       "      <td>-0.120657</td>\n",
       "      <td>-0.117846</td>\n",
       "      <td>-0.116706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093707</td>\n",
       "      <td>0.093835</td>\n",
       "      <td>0.099491</td>\n",
       "      <td>0.100521</td>\n",
       "      <td>0.102367</td>\n",
       "      <td>0.110598</td>\n",
       "      <td>0.11524</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.154079</td>\n",
       "      <td>0.162401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      worst     waste     awful       bad    boring      poor    poorly  \\\n",
       "0 -0.241149 -0.189228 -0.168824 -0.167629 -0.153832 -0.128468 -0.120763   \n",
       "\n",
       "      worse  terrible  disappointment    ...          fun     today  favorite  \\\n",
       "0 -0.120657 -0.117846       -0.116706    ...     0.093707  0.093835  0.099491   \n",
       "\n",
       "    amazing     loved  wonderful     best  perfect  excellent     great  \n",
       "0  0.100521  0.102367   0.110598  0.11524   0.1201   0.154079  0.162401  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(grid.best_estimator_.named_steps['logisticregression'].coef_.reshape(-1,1), index = feature_names)\n",
    "df2.sort_values(by = 0,inplace= True)\n",
    "pd.concat([df2[:20], df2[-20:]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.90576,\n",
       " {'logisticregression__C': 10, 'tfidfvectorizer__ngram_range': (1, 3)})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df = 5),LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "             'tfidfvectorizer__ngram_range':[(1,1), (1,2), (1,3)]}\n",
    "\n",
    "X_train = df[df['y_train']!=2]['X_train']\n",
    "grid = GridSearchCV(pipe, param_grid, cv = 5).fit(X_train, y_train)\n",
    "grid.best_score_, grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>worst</th>\n",
       "      <th>awful</th>\n",
       "      <th>boring</th>\n",
       "      <th>the worst</th>\n",
       "      <th>poor</th>\n",
       "      <th>waste</th>\n",
       "      <th>terrible</th>\n",
       "      <th>worse</th>\n",
       "      <th>no</th>\n",
       "      <th>...</th>\n",
       "      <th>the best</th>\n",
       "      <th>loved</th>\n",
       "      <th>best</th>\n",
       "      <th>today</th>\n",
       "      <th>fun</th>\n",
       "      <th>amazing</th>\n",
       "      <th>wonderful</th>\n",
       "      <th>perfect</th>\n",
       "      <th>excellent</th>\n",
       "      <th>great</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-13.628529</td>\n",
       "      <td>-13.555011</td>\n",
       "      <td>-11.964566</td>\n",
       "      <td>-11.358971</td>\n",
       "      <td>-10.358862</td>\n",
       "      <td>-10.282947</td>\n",
       "      <td>-9.535128</td>\n",
       "      <td>-8.478191</td>\n",
       "      <td>-8.356218</td>\n",
       "      <td>-8.071425</td>\n",
       "      <td>...</td>\n",
       "      <td>6.17336</td>\n",
       "      <td>6.187728</td>\n",
       "      <td>6.426293</td>\n",
       "      <td>6.984822</td>\n",
       "      <td>7.226938</td>\n",
       "      <td>7.339721</td>\n",
       "      <td>8.698145</td>\n",
       "      <td>9.022522</td>\n",
       "      <td>10.519394</td>\n",
       "      <td>13.034443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         bad      worst      awful     boring  the worst       poor     waste  \\\n",
       "0 -13.628529 -13.555011 -11.964566 -11.358971 -10.358862 -10.282947 -9.535128   \n",
       "\n",
       "   terrible     worse        no    ...      the best     loved      best  \\\n",
       "0 -8.478191 -8.356218 -8.071425    ...       6.17336  6.187728  6.426293   \n",
       "\n",
       "      today       fun   amazing  wonderful   perfect  excellent      great  \n",
       "0  6.984822  7.226938  7.339721   8.698145  9.022522  10.519394  13.034443  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "coef = grid.best_estimator_.named_steps['logisticregression'].coef_\n",
    "df3 = pd.DataFrame(coef.reshape(-1,1), index = feature_names)\n",
    "df3.sort_values(by = 0,inplace= True)\n",
    "pd.concat([df3[:20], df3[-20:]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>of the worst</th>\n",
       "      <td>-5.487569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waste of time</th>\n",
       "      <td>-3.093326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supposed to be</th>\n",
       "      <td>-2.710675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none of the</th>\n",
       "      <td>-2.536097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the worst movie</th>\n",
       "      <td>-2.397477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to sit through</th>\n",
       "      <td>-2.380264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first of all</th>\n",
       "      <td>-2.297215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not one of</th>\n",
       "      <td>-2.289769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>some kind of</th>\n",
       "      <td>-2.263779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of the movie</th>\n",
       "      <td>-2.254693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if they are</th>\n",
       "      <td>-2.151812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the only good</th>\n",
       "      <td>-2.143980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nothing more than</th>\n",
       "      <td>-2.090583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to work with</th>\n",
       "      <td>-2.085418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of my life</th>\n",
       "      <td>-2.084076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>better than this</th>\n",
       "      <td>-2.012219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with this film</th>\n",
       "      <td>-2.004692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the worst film</th>\n",
       "      <td>-2.000627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if you want</th>\n",
       "      <td>-1.991339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would not recommend</th>\n",
       "      <td>-1.976921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is the worst</th>\n",
       "      <td>-1.962068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wanted to like</th>\n",
       "      <td>-1.955624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not very good</th>\n",
       "      <td>-1.950224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want to see</th>\n",
       "      <td>-1.911289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waste your time</th>\n",
       "      <td>-1.906210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on and on</th>\n",
       "      <td>-1.878866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is not good</th>\n",
       "      <td>-1.876048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this film just</th>\n",
       "      <td>-1.842806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the worst movies</th>\n",
       "      <td>-1.838021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was very disappointed</th>\n",
       "      <td>-1.812480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what to expect</th>\n",
       "      <td>1.754185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is very good</th>\n",
       "      <td>1.756907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can wait to</th>\n",
       "      <td>1.790974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the first time</th>\n",
       "      <td>1.791252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this was the</th>\n",
       "      <td>1.793790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>some of the</th>\n",
       "      <td>1.870895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love this movie</th>\n",
       "      <td>1.940394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the best movie</th>\n",
       "      <td>1.943034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would love to</th>\n",
       "      <td>1.955033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what it is</th>\n",
       "      <td>1.959322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>don want to</th>\n",
       "      <td>2.011058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lot of fun</th>\n",
       "      <td>2.022103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>check it out</th>\n",
       "      <td>2.032389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as much as</th>\n",
       "      <td>2.051606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loved this movie</th>\n",
       "      <td>2.054395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this is great</th>\n",
       "      <td>2.057786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but it is</th>\n",
       "      <td>2.063171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of the most</th>\n",
       "      <td>2.073836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if you don</th>\n",
       "      <td>2.239929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may not be</th>\n",
       "      <td>2.244222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly recommend this</th>\n",
       "      <td>2.306848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 out of</th>\n",
       "      <td>2.313689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it was very</th>\n",
       "      <td>2.379559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was pretty good</th>\n",
       "      <td>2.405479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>must see for</th>\n",
       "      <td>2.405894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at the end</th>\n",
       "      <td>2.469339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one of my</th>\n",
       "      <td>2.754981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much better than</th>\n",
       "      <td>2.780809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is the best</th>\n",
       "      <td>2.900249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of the best</th>\n",
       "      <td>4.031992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95030 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0\n",
       "of the worst          -5.487569\n",
       "waste of time         -3.093326\n",
       "supposed to be        -2.710675\n",
       "none of the           -2.536097\n",
       "the worst movie       -2.397477\n",
       "to sit through        -2.380264\n",
       "first of all          -2.297215\n",
       "not one of            -2.289769\n",
       "some kind of          -2.263779\n",
       "of the movie          -2.254693\n",
       "if they are           -2.151812\n",
       "the only good         -2.143980\n",
       "nothing more than     -2.090583\n",
       "to work with          -2.085418\n",
       "of my life            -2.084076\n",
       "better than this      -2.012219\n",
       "with this film        -2.004692\n",
       "the worst film        -2.000627\n",
       "if you want           -1.991339\n",
       "would not recommend   -1.976921\n",
       "is the worst          -1.962068\n",
       "wanted to like        -1.955624\n",
       "not very good         -1.950224\n",
       "want to see           -1.911289\n",
       "waste your time       -1.906210\n",
       "on and on             -1.878866\n",
       "is not good           -1.876048\n",
       "this film just        -1.842806\n",
       "the worst movies      -1.838021\n",
       "was very disappointed -1.812480\n",
       "...                         ...\n",
       "what to expect         1.754185\n",
       "is very good           1.756907\n",
       "can wait to            1.790974\n",
       "the first time         1.791252\n",
       "this was the           1.793790\n",
       "some of the            1.870895\n",
       "love this movie        1.940394\n",
       "the best movie         1.943034\n",
       "would love to          1.955033\n",
       "what it is             1.959322\n",
       "don want to            2.011058\n",
       "lot of fun             2.022103\n",
       "check it out           2.032389\n",
       "as much as             2.051606\n",
       "loved this movie       2.054395\n",
       "this is great          2.057786\n",
       "but it is              2.063171\n",
       "of the most            2.073836\n",
       "if you don             2.239929\n",
       "may not be             2.244222\n",
       "highly recommend this  2.306848\n",
       "10 out of              2.313689\n",
       "it was very            2.379559\n",
       "was pretty good        2.405479\n",
       "must see for           2.405894\n",
       "at the end             2.469339\n",
       "one of my              2.754981\n",
       "much better than       2.780809\n",
       "is the best            2.900249\n",
       "of the best            4.031992\n",
       "\n",
       "[95030 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.array([len(feature.split()) for feature in df3.index.values])==3\n",
    "df3[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced tokenization, stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "# lemmatization\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def compare_normalization(doc):\n",
    "    doc_spacy = en_nlp(doc)\n",
    "    print('Lmmatization:')\n",
    "    print([token.lemma_ for token in doc_spacy])\n",
    "    print('Stemming:')\n",
    "    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lmmatization:\n",
      "['-PRON-', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', \"i'm\", 'scar', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\n",
      "Stemming:\n",
      "['our', 'meet', 'today', 'wa', 'wors', 'than', 'yesterday', ',', \"i'm\", 'scare', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "compare_normalization(u\"Our meeting today was worse than yesterday,\" \"I'm scared of meeting the clients tomorrow.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
